[{"title":"精读——AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION ATSCALE","url":"/2025/04/26/AN-IMAGE-IS-WORTH-16X16-WORDS/","content":"\n  \n    \n      INTRODUCTION\n\n    \n    \n      Self-Attention在NLP大放异彩，在CV（computer vision）领域要怎么应用呢？\n\n    \n  \n\n首选，图片相对于NLP处理的序列太大了，相关的工作有两个方向：\n\nCNN和Self-Attention结合使用（比如可以用ResNet的特征图作为输入）\nStand-Alone Self-Attention(用一个局部的window缩小计算复杂度) / Axial Attention（分别拆开高度和宽度做Self-Attention）\n\nVison Transformer 将图像拆分为 16x16 的图块（Patch），视为“单词”输入 Transformer，通过 全局自注意力（Self-Attention） 直接建模所有图块间关系，无需卷积。\nViT在中等模型上，如果没有比较强的约束 ，是要比同等级别的ResNet要弱几个百分点的。作者解释：\n\n Transformers 缺少一些归纳偏置（Inductive biases）（如：Locality（邻近则相关）， Translation Equivariance（操作顺序无关））\n\n然而在大规模数据集上，ViT有着显著优势。\n\n  \n    \n      CONCLUSION\n\n    \n    \n      不需要图像特有的Inductive biases，可以直接把图片分块后当作word处理，用NLP的Transformer来做图像的分类。在有大规模预训练后，效果很好。\n文章提出的问题：Vison Transformer能不能在分割和检测上发挥作用。\n后续：检测方面：ViT-FRCNN\n​\t   分割方面：SETR\n​\t   Swin Transformer、ViT-G\n\n    \n  \n\n\n\n\n  \n    \n      RELATED-WORK\n\n    \n    \n      如何把自注意力机制用在图片上？\n\n把像素当成一个元素（复杂度太高）\n不用整张图，用local neighborhoods做自注意力\nSparse Transformers对一些稀疏的点做自注意力，是全局注意力的近似\n\n\n    \n  \n\n\n\n\n  \n    \n      METHOD\n\n    \n    \n      Vision Transformer(ViT)\n\n\n    \n  \n\n\n将图片分成Patch，加入Position Embedding，变成一个序列，通过Linear Projecttion of Flattened Patches\n将这个加工过的序列放入Transformer Encoder\n分类通过Extra Learnable embedding（参考BERT）加入一个特殊字符CLS，因为所有的Token和Token之间都在做交互，我们相信Class Embedding能够从别的Embedding学到有用的信息。最后根据CLS做最后的判断\n进入MLP Head 用交叉熵函数进行训练。\n\n前向过程：\n为输入， 如果使用的Patch可以得到个维度为768图像块($196\\times768(16163)$).\n经过全连接层（维度为）,经过（线性投射层）维度变为加上,整体进入Transformer Encoder的维度为(位置编码是相加的，不改变维度)。、\n进入Transformer Encoder的KQV（如果有12个头的话，整体的维度变为）合起来还是 MLP会放大，然后缩小。\n","categories":["论文精读","ICLR"],"tags":["Transformer","Image Classification","ViT","ICLR","CV","论文精读"]},{"title":"精读——Deep Residual Learning for Image Recognition","url":"/2025/04/24/%E3%80%8ADeep-Residual-Learning-for-Image-Recognition%E3%80%8B%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/","content":"\n有一个反直觉的事情：就是为什么深度越深，网络的表现反而越差呢？ResNet解决的就是这个问题。\n\n\n  \n    \n      Introduction\n\n    \n    \n      Is learning better networks as easy as stacking more layers?\n\n    \n  \n\n传统的答案是，网路很深的时候，会出现梯度消失/梯度爆炸（vanishing gradients / exploding gradients）.\n在ResNet出现前，通常可以在权重初始化时，不要过大或者过小，同时可以在中间层加入一些normalization layers，使得校验每个层之间的输出、均值和方差。 \n\n在上图中，可以看到deeper network has higher training error，这不是过拟合（测试误差也很大），这个很反直觉：因为我们至少应该是输入  输出也是  的identity mapping，但是SGD找不到这个解（Identity Mapping）. 本文就提出了新的方法，使得显式的构造一个identity mapping ， 让深的网络不会比浅的网络差，作者提出 deep residual learning framework，即深度残差学习网络。\nDeep Residual Learning Framework 的基础理解\n浅的网络输出  经过残差学习，最终输出:\n这样有两个好处：\n\n不会增加模型复杂度，没有额外的参数要学习。 \n计算也不会更复杂，因为只是一个加法而已。\n\n\n  \n    \n      Related-Work\n\n    \n    \n      Is learning better networks as easy as stacking more layers?\n\n    \n  \n\n\n\nResidual Network\n 如何处理输入形状和输出形状不同的情况？\n\n本文提到了两种方法：\n\n输入和输出添加额外的0，使得可以相加\n投影：通过的卷积层，使得输出通道是输入通道的两倍。\n\nBatch Normalization目的是使feature map满足均值为1，方差为0的分布规律。\nExperiments","categories":["论文精读","CVPR"],"tags":["Image Classification","ViT","ICLR","论文精读","ResNet"]},{"title":"CVPR相关比赛调研","url":"/2025/04/28/%E6%AF%94%E8%B5%9B%E8%B0%83%E7%A0%94/","content":"Ego4dEpisodic memory（情景记忆）Moments queries (MQ)比赛基本信息BaseLine代码链接\n比赛链接\n\nhttps://eval.ai/web/challenges/challenge-page/1626/overview\n\n比赛内容\n\n主要任务\nThis task aims to query an egocentric video based on a category of actions. Specifically, it poses the following request ‘Retrieve all the moments that I do X in the video.’, where ‘X’ comes from a pre-defined taxonomy of action categories, such as ‘interact with someone’ or ‘use phone’. Given an input video and a query action category, the goal is to retrieve all the instances of this action category in the video.\n即：输入一个视频片段和查询动作，输出该动作的起始和结束时间。\n\n\n比赛举办机构\n\nEgo4D2023冠军代码（目前榜单第四）：https://github.com/JonnyS1226/ego4d_asl\n2023冠军论文：https://arxiv.org/abs/2306.09172\n2024代码未公开，仓库在：https://github.com/OpenGVLab/EgoVideo\n\nGoal Step比赛基本信息BaseLine代码链接\n比赛链接\n\nhttps://eval.ai/web/challenges/challenge-page/2188/overview\n\n比赛内容\n\n主要任务\nThere are three tasks of interest in Ego4D Goal-Step (1) Goal/Step localization (2) Online goal/step detection (3) Step grounding. In this challenge, we focus on Step grounding. Given an untrimmed egocentric video, identify the temporal action segment corresponding to a natural language description of the step. Specifically, predict the (start_time, end_time) for a given keystep description.\n即：focus on Step grounding ，预测给定的关键步骤的开始和结束时间。\n\n比赛举办机构\n\nEgo4D暂时没找到\n\n\n\nEgo Schema比赛基本信息BaseLine代码链接\n比赛链接\n\nhttps://eval.ai/web/challenges/challenge-page/2238/overview\n\n比赛内容\n\n主要任务\nWe introduce EgoSchema, a very long-form video question-answering benchmark to evaluate long video understanding capabilities of modern vision and language systems. Derived from Ego4D, EgoSchema consists of over 5000 human curated multiple choice question answer pairs, spanning over 250 hours of real video data, covering a very broad range of natural human activity and behavior. For each question, EgoSchema requires the correct answer to be selected between five given options based on a three-minute-long video clip. More details at our website and Github page.\n\n即：给定一个3分钟的视频切片，一个问题和5个选项，从五个选择中选择最正确的选项。\n\n比赛举办机构\n\nEgo4D2024冠军代码：https://github.com/Hyu-Zhang/HCQA\n2024冠军论文：https://arxiv.org/abs/2406.15771\n\n\n\nSocial UnderstandingLooking at me比赛基本信息BaseLine代码链接\n比赛链接\n\nhttps://eval.ai/web/challenges/challenge-page/1624/overview\n\n比赛内容\n\n主要任务\nAn egocentric video provides a unique lens for studying social interactions because it captures utterances and nonverbal cues from each participant’s unique view and enables embodied approaches to social understanding. Progress in egocentric social understanding could lead to more capable virtual assistants and social robots. Computational models of social interactions can also provide new tools for diagnosing and treating disorders of socialization and communication such as autism, and could support novel prosthetic technologies for the hearing-impaired.\nWhile the Ego4D dataset can support such a long-term research agenda, our looking-at-me task focuses on identifying communicative acts that are directed towards the camera-wearer, as distinguished from those directed to other social partners: given a video in which the faces of social partners have been localized and identified, classify whether each visible face is looking at the camera wearer.\n即：给定地面状况（用唯一ID跨帧追踪每个人脸，对于每个可见的人，标注了看向摄像头佩戴者的时间段），输出对于每一帧是否看向了摄像头的佩戴者（二进制）。\n\n\n比赛举办机构\n\nEgo4D暂时没找到\n\nTalking to me比赛基本信息BaseLine代码链接\n比赛链接\n\nhttps://eval.ai/web/challenges/challenge-page/2238/overview\n\n比赛内容\n\n主要任务\nAn egocentric video provides a unique lens for studying social interactions because it captures utterances and nonverbal cues from each participant’s unique view and enables embodied approaches to social understanding. Progress in egocentric social understanding could lead to more capable virtual assistants and social robots. Computational models of social interactions can also provide new tools for diagnosing and treating disorders of socialization and communication such as autism, and could support novel prosthetic technologies for the hearing-impaired.\nWhile the Ego4D dataset can support such a long-term research agenda, our initial Social benchmark focuses on multimodal understanding of conversational interactions via attention and speech. Specifically, we focus on identifying communicative acts that are directed towards the camera-wearer, as distinguished from those directed to other social partners: Talking to me (TTM): given a video and audio segment with the same tracked faces and an additional label that identifies speaker status, classify whether each visible face is talking to the camera wearer. he TTM task is defined as a frame-level prediction y, which stands in contrast to audio analysis tasks where labels are often assigned at the level of audio frames or segments. A desired model must be able to make a consolidated decision based on the video and audio cues over the time course of an utterance. For example, if the speaker turns their head to the side momentarily while speaking to the camera-wearer, then a frame where the speaker is looking away would have y = 1.\n\n即：给定视频和音频片段且该说话者已被标注为正在说话，输出对于每一帧是否对摄像头佩戴者说话（二进制）。\n\n比赛举办机构\n\nEgo4D2023冠军代码：https://github.com/hsi-che-lin/Ego4D-QuAVF-TTM-CVPR23\n2023冠军论文：https://arxiv.org/abs/2306.17404\n\n\n\nForecasting","categories":["cvpr"],"tags":["cvpr"]}]