[{"title":"精读——AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION ATSCALE","url":"/2025/04/26/AN-IMAGE-IS-WORTH-16X16-WORDS/","content":"\n  \n    \n      INTRODUCTION\n\n    \n    \n      Self-Attention在NLP大放异彩，在CV（computer vision）领域要怎么应用呢？\n\n    \n  \n\n首选，图片相对于NLP处理的序列太大了，相关的工作有两个方向：\n\nCNN和Self-Attention结合使用（比如可以用ResNet的特征图作为输入）\nStand-Alone Self-Attention(用一个局部的window缩小计算复杂度) / Axial Attention（分别拆开高度和宽度做Self-Attention）\n\nVison Transformer 将图像拆分为 16x16 的图块（Patch），视为“单词”输入 Transformer，通过 全局自注意力（Self-Attention） 直接建模所有图块间关系，无需卷积。\nViT在中等模型上，如果没有比较强的约束 ，是要比同等级别的ResNet要弱几个百分点的。作者解释：\n\n Transformers 缺少一些归纳偏置（Inductive biases）（如：Locality（邻近则相关）， Translation Equivariance（操作顺序无关））\n\n然而在大规模数据集上，ViT有着显著优势。\n\n  \n    \n      CONCLUSION\n\n    \n    \n      不需要图像特有的Inductive biases，可以直接把图片分块后当作word处理，用NLP的Transformer来做图像的分类。在有大规模预训练后，效果很好。\n文章提出的问题：Vison Transformer能不能在分割和检测上发挥作用。\n后续：检测方面：ViT-FRCNN\n​\t   分割方面：SETR\n​\t   Swin Transformer、ViT-G\n\n    \n  \n\n\n\n\n  \n    \n      RELATED-WORK\n\n    \n    \n      如何把自注意力机制用在图片上？\n\n把像素当成一个元素（复杂度太高）\n不用整张图，用local neighborhoods做自注意力\nSparse Transformers对一些稀疏的点做自注意力，是全局注意力的近似\n\n\n    \n  \n\n\n\n\n  \n    \n      METHOD\n\n    \n    \n      Vision Transformer(ViT)\n\n\n    \n  \n\n\n将图片分成Patch，加入Position Embedding，变成一个序列，通过Linear Projecttion of Flattened Patches\n将这个加工过的序列放入Transformer Encoder\n分类通过Extra Learnable embedding（参考BERT）加入一个特殊字符CLS，因为所有的Token和Token之间都在做交互，我们相信Class Embedding能够从别的Embedding学到有用的信息。最后根据CLS做最后的判断\n进入MLP Head 用交叉熵函数进行训练。\n\n前向过程：\n为输入， 如果使用的Patch可以得到个维度为768图像块($196\\times768(16163)$).\n经过全连接层（维度为）,经过（线性投射层）维度变为加上,整体进入Transformer Encoder的维度为(位置编码是相加的，不改变维度)。、\n进入Transformer Encoder的KQV（如果有12个头的话，整体的维度变为）合起来还是 MLP会放大，然后缩小。\n","categories":["论文精读","ICLR"],"tags":["Transformer","Image Classification","ViT","ICLR","CV","论文精读"]},{"title":"精读——Deep Residual Learning for Image Recognition","url":"/2025/04/24/%E3%80%8ADeep-Residual-Learning-for-Image-Recognition%E3%80%8B%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/","content":"\n有一个反直觉的事情：就是为什么深度越深，网络的表现反而越差呢？ResNet解决的就是这个问题。\n\n\n  \n    \n      Introduction\n\n    \n    \n      Is learning better networks as easy as stacking more layers?\n\n    \n  \n\n传统的答案是，网路很深的时候，会出现梯度消失/梯度爆炸（vanishing gradients / exploding gradients）.\n在ResNet出现前，通常可以在权重初始化时，不要过大或者过小，同时可以在中间层加入一些normalization layers，使得校验每个层之间的输出、均值和方差。 \n\n在上图中，可以看到deeper network has higher training error，这不是过拟合（测试误差也很大），这个很反直觉：因为我们至少应该是输入  输出也是  的identity mapping，但是SGD找不到这个解（Identity Mapping）. 本文就提出了新的方法，使得显式的构造一个identity mapping ， 让深的网络不会比浅的网络差，作者提出 deep residual learning framework，即深度残差学习网络。\nDeep Residual Learning Framework 的基础理解\n浅的网络输出  经过残差学习，最终输出:\n这样有两个好处：\n\n不会增加模型复杂度，没有额外的参数要学习。 \n计算也不会更复杂，因为只是一个加法而已。\n\n\n  \n    \n      Related-Work\n\n    \n    \n      Is learning better networks as easy as stacking more layers?\n\n    \n  \n\n\n\nResidual Network\n 如何处理输入形状和输出形状不同的情况？\n\n本文提到了两种方法：\n\n输入和输出添加额外的0，使得可以相加\n投影：通过的卷积层，使得输出通道是输入通道的两倍。\n\nBatch Normalization目的是使feature map满足均值为1，方差为0的分布规律。\nExperiments","categories":["论文精读","CVPR"],"tags":["Image Classification","ViT","ICLR","论文精读","ResNet"]}]