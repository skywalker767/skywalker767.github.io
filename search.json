[{"title":"精读——Deep Residual Learning for Image Recognition","url":"/2025/04/24/%E3%80%8ADeep-Residual-Learning-for-Image-Recognition%E3%80%8B%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/","content":"\n有一个反直觉的事情：就是为什么深度越深，网络的表现反而越差呢？ResNet解决的就是这个问题。\n\n\n  \n    \n      Introduction\n\n    \n    \n      Is learning better networks as easy as stacking more layers?\n\n    \n  \n\n传统的答案是，网路很深的时候，会出现梯度消失/梯度爆炸（vanishing gradients / exploding gradients）.\n在ResNet出现前，通常可以在权重初始化时，不要过大或者过小，同时可以在中间层加入一些normalization layers，使得校验每个层之间的输出、均值和方差。 \n\n在上图中，可以看到deeper network has higher training error，这不是过拟合（测试误差也很大），这个很反直觉：因为我们至少应该是输入  输出也是  的identity mapping，但是SGD找不到这个解（Identity Mapping）. 本文就提出了新的方法，使得显式的构造一个identity mapping ， 让深的网络不会比浅的网络差，作者提出 deep residual learning framework，即深度残差学习网络。\nDeep Residual Learning Framework 的基础理解\n浅的网络输出  经过残差学习，最终输出:\n这样有两个好处：\n\n不会增加模型复杂度，没有额外的参数要学习。 \n计算也不会更复杂，因为只是一个加法而已。\n\n\n  \n    \n      Related-Work\n\n    \n    \n      Is learning better networks as easy as stacking more layers?\n\n    \n  \n\n\n\nResidual Network\n 如何处理输入形状和输出形状不同的情况？\n\n本文提到了两种方法：\n\n输入和输出添加额外的0，使得可以相加\n投影：通过的卷积层，使得输出通道是输入通道的两倍。\n\nBatch Normalization目的是使feature map满足均值为1，方差为0的分布规律。\nExperiments","categories":["论文精读","CVPR"],"tags":["论文精读","ResNet","CVPR"]}]